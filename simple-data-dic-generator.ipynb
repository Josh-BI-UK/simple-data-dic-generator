{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Dictionary Generator\n",
    "This Python script analyses a \"source CSV\" file and provides basic data analysis information for each field.\n",
    "\n",
    "It is then possible to download and parse the summary table (in JSON format) into Excel and create a standard table from it. The goal is to develop a simple method for creating the foundation of a data dictionary for data warehouse and ETL projects.\n",
    "\n",
    "<br>\n",
    "\n",
    "The summary table contains the following columns:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **Column Name:** Name of a column (i.e. header) within the \"source CSV\" file.\n",
    "\n",
    "2. **Sample Data:** A list of values, (a maximum of ten values), randomly sampled from within a column.  Each value should be separated by a semicolon.\n",
    "\n",
    "3. **Min Value:** Minimum value within a column, only if the data type is \"Number\" or \"Date\", else for any other data type state \"n-a\". If all values within the column are nulls, then state \"n-a\".\n",
    "\n",
    " 4. **Max Value:** Maximum value within a column, only if the data type is \"Number\" or \"Date\", else for any other data type state \"n-a\". If all values within the column are nulls, then state \"n-a\".\n",
    "\n",
    " 5. **Data Type:** Using basic data types to analyse all of the values within a column. Data types and mappings to Python-style data types are as follows:\n",
    "\n",
    "    - Unicode values: \"Text\"\n",
    "    - String or text values: \"Text\"\n",
    "    - Mixed numerical and non-numerical values: \"Text\"\n",
    "    - Date and time values: \"Date\"\n",
    "    - Integer numbers: \"Number\"\n",
    "    - Floating point numbers: \"Number\"\n",
    "    - True/False values: \"Boolean\"\n",
    "    - Column with all NULLs: \"All Nulls\"\n",
    "\n",
    "<br>\n",
    "\n",
    "6. **Max Character Length:** If data type is \"Text\", the length of the shortest string value in a column, else for any other data type state \"n-a\". If all values within the column are nulls, then state \"n-a\".\n",
    "\n",
    "7. **Min Character Length:** If data type is \"Text\", the length of the shortest string value in a column, else for any other data type state \"n-a\". If all values within the column are nulls, then state \"n-a\".\n",
    "\n",
    "8. **% NULL Fields:** Percentage of values which are nulls within a column, formatted to two decimal places. If no nulls are present, state 0%.\n",
    "\n",
    "9. **NULL field count:** Count of values which are NOT nulls within a column.\n",
    "\n",
    "10. **Not NULL field count:** Count of values which are nulls within a column.\n",
    "\n",
    "11. **Count of Unique Values:** Count unique values within a column. If all values are null, then state \"n-a\".\n",
    "\n",
    "<br>\n",
    "\n",
    "The code should implement the following requirements:\n",
    "\n",
    "a) Each row of the \"output\" table should represent one column from the source CSV.\n",
    "\n",
    "b) For data sets with more than 400,000 rows (this figure can be changed via the variable 'MAX_ROWS'), the analysis will be based on a random sample, equal to MAX_ROWS (default = 400k rows).\n",
    "\n",
    "c) The \"output\" JSON can be easily parsed as a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file paths\n",
    "source_csv = \"https://github.com/<replace_with_link_to_input_csv>.csv?raw=true\"\n",
    "output_directory = \"/Users/<replace-with-your-output-path>\"\n",
    "output_file_name = \"<replace-with-name-ofoutput-csv>.json\"\n",
    "output_json_file = os.path.join(output_directory, output_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "output_directory = os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Set maximum number of rows to read from CSV\n",
    "MAX_ROWS = 400000\n",
    "\n",
    "# Initialize an empty list to store the column information\n",
    "columns_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a Pandas dataframe\n",
    "df = pd.read_csv(source_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataframe has more than MAX_ROWS variable\n",
    "if len(df) > MAX_ROWS:\n",
    "    # If it does, take a random sample equal to MAX_ROWS variable\n",
    "    df = df.sample(n=MAX_ROWS, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each column in the dataframe\n",
    "for col in df.columns:\n",
    "    # Get the column data\n",
    "    col_data = df[col]\n",
    "    unique_col_data = df[col].drop_duplicates()\n",
    "\n",
    "    # Get a sample of the column data\n",
    "    sample_data = \";\".join([str(val) for val in col_data.sample(n=min(10, len(col_data)), random_state=1).tolist()])\n",
    "    unique_sample_data = \"; \".join([str(val) for val in unique_col_data.sample(n=min(10, len(unique_col_data)), random_state=1).tolist()])\n",
    "    \n",
    "    # Get the data type of the column\n",
    "    data_type = str(col_data.dtype)\n",
    "\n",
    "    # Initialize the null count and unique value set\n",
    "    null_count = 0\n",
    "    unique_vals = set()\n",
    "\n",
    "    # Loop through each value in the column\n",
    "    for val in col_data:\n",
    "        # If the value is null, increment the null count\n",
    "        if pd.isnull(val):\n",
    "            null_count += 1\n",
    "        # If the value is not null, add it to the unique value set\n",
    "        else:\n",
    "            unique_vals.add(val)\n",
    "\n",
    "    # Test if all values in coluimn are nulls\n",
    "    col_all_null_test = col_data.isnull().all()\n",
    "\n",
    "    # Calculate the percentage of nulls and the count of unique values\n",
    "    null_percent = round(null_count / len(col_data) * 100, 2)\n",
    "    unique_count = len(unique_vals)\n",
    "\n",
    "# Determine data type of column\n",
    "    col_drop_na = df.dropna(axis=0,subset=col)\n",
    "    data_type = col_drop_na\n",
    "    # data_type = col_data.dtype\n",
    "    data_type_str = str(data_type)\n",
    "\n",
    "# Map Python data types against basic set of data types as follows:\n",
    "\n",
    "    # - Unicode values: \"Text\"\n",
    "    # - String or text values: \"Text\"\n",
    "    # - Mixed numeric and non-numeric values: \"Text\"\n",
    "    # - Date and time values: \"Date\"\n",
    "    # - Integer numbers: \"Number\"\n",
    "    # - Floating point numbers: \"Number\"\n",
    "    # - True/False values: \"Boolean\"\n",
    "    # - Column with all NULLs: \"All Nulls\n",
    "\n",
    "    if data_type == np.dtype('O'):\n",
    "        # Column contains strings or mixed numerical and non-numerical values\n",
    "        char_lengths = [len(str(val)) for val in col_data if not pd.isnull(val)]\n",
    "        data_type_str = \"Text\"\n",
    "        min_val = \"n-a\"\n",
    "        max_val = \"n-a\"\n",
    "        min_char_length = min(char_lengths)\n",
    "        max_char_length = max(char_lengths)\n",
    "\n",
    "    elif np.issubdtype(data_type, np.number):\n",
    "        # Column contains numeric values\n",
    "        data_type_str = \"Number\"\n",
    "        min_val = str(col_data.min())\n",
    "        max_val = str(col_data.max())\n",
    "        min_char_length = \"n-a\"\n",
    "        max_char_length = \"n-a\"\n",
    "\n",
    "    elif data_type == 'bool':\n",
    "        # Column contains True/False values\n",
    "        data_type_str = \"Boolean\"\n",
    "        min_val = \"n-a\"\n",
    "        max_val = \"n-a\"\n",
    "        min_char_length = \"n-a\"\n",
    "        max_char_length = \"n-a\"\n",
    "\n",
    "    elif np.issubdtype(data_type, np.datetime64):\n",
    "        # Column contains date/time values\n",
    "        data_type_str =  \"Date/Time\"\n",
    "        min_val = col_data.min().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        max_val = col_data.max().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        min_char_length = \"n-a\"\n",
    "        max_char_length = \"n-a\"\n",
    "\n",
    "    else:\n",
    "        # Column contains unknown data type\n",
    "        data_type_str = \"Unknown\"\n",
    "        min_val = \"n-a\"\n",
    "        max_val = \"n-a\"\n",
    "        min_char_length = \"n-a\"\n",
    "        max_char_length = \"n-a\"\n",
    "\n",
    "\n",
    "    # Create a dictionary with the column information\n",
    "    col_info = {\n",
    "        \"Ordinal Position (Zero based)\": df.columns.get_loc(col),\n",
    "        \"Column Name\": col,\n",
    "        \"Sample Data\": unique_sample_data,\n",
    "        \"Min Value\": min_val,\n",
    "        \"Max Value\": max_val,\n",
    "        \"Data Type\": data_type_str,\n",
    "        \"Max Character Length\": max_char_length,\n",
    "        \"Min Character Length\": min_char_length,\n",
    "        \"Column All Nulls\": \"All null\" if col_all_null_test == True else \"No\",\n",
    "        \"% NULL fields\": f\"{null_percent:.2f}%\",\n",
    "        \"NULL field count\": null_count,\n",
    "        \"Count of Unique Values\": unique_count }\n",
    "    \n",
    "    # print(col_info)\n",
    "    # Append col_info iteration to main col_information list\n",
    "    columns_info.append(col_info.copy())\n",
    "\n",
    "# Convert list of dictionaries to JSON file\n",
    "with open(output_json_file, 'w') as json_file:\n",
    "    json.dump(columns_info, json_file, ensure_ascii=False, indent=4, separators=(',',': '))\n",
    "\n",
    "json_file.close()\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
